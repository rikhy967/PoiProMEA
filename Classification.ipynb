{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data after PP/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mu1</th>\n",
       "      <th>lam1</th>\n",
       "      <th>mu2</th>\n",
       "      <th>sigma</th>\n",
       "      <th>mu3</th>\n",
       "      <th>sigma3</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>w3</th>\n",
       "      <th>n_spikes</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>1.376800e-01</td>\n",
       "      <td>0.069408</td>\n",
       "      <td>0.368091</td>\n",
       "      <td>0.172628</td>\n",
       "      <td>0.286221</td>\n",
       "      <td>4.179009e-01</td>\n",
       "      <td>2.958779e-01</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019293</td>\n",
       "      <td>0.019831</td>\n",
       "      <td>6.633874e-02</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>10364.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043575</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>1.779872e-01</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100064</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>4589.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.022119</td>\n",
       "      <td>6.418069e-02</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>8274.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048184</td>\n",
       "      <td>0.030814</td>\n",
       "      <td>4.696582e-03</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100139</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>4151.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.011880</td>\n",
       "      <td>1.038845e-01</td>\n",
       "      <td>0.058294</td>\n",
       "      <td>0.483309</td>\n",
       "      <td>0.271518</td>\n",
       "      <td>0.332834</td>\n",
       "      <td>4.549999e-01</td>\n",
       "      <td>2.121657e-01</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>1.807094e-01</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>10784.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>0.099969</td>\n",
       "      <td>0.039687</td>\n",
       "      <td>9.343772e-02</td>\n",
       "      <td>0.040354</td>\n",
       "      <td>0.267501</td>\n",
       "      <td>0.112234</td>\n",
       "      <td>0.630343</td>\n",
       "      <td>2.080073e-01</td>\n",
       "      <td>1.616492e-01</td>\n",
       "      <td>1523.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>0.056240</td>\n",
       "      <td>0.031443</td>\n",
       "      <td>1.643654e-07</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.100001</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.192093e-07</td>\n",
       "      <td>1.421085e-14</td>\n",
       "      <td>3555.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.008357e-01</td>\n",
       "      <td>0.044780</td>\n",
       "      <td>0.403570</td>\n",
       "      <td>0.174828</td>\n",
       "      <td>0.774085</td>\n",
       "      <td>1.422010e-01</td>\n",
       "      <td>8.371432e-02</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mu1      lam1           mu2     sigma       mu3    sigma3        w1  \\\n",
       "0    0.099999  0.010000  1.376800e-01  0.069408  0.368091  0.172628  0.286221   \n",
       "1    0.019293  0.019831  6.633874e-02  0.000100  0.100001  0.000520  1.000000   \n",
       "2    0.043575  0.029764  1.779872e-01  0.000100  0.100064  0.000260  1.000000   \n",
       "3    0.024164  0.022119  6.418069e-02  0.000100  0.100010  0.000718  1.000000   \n",
       "4    0.048184  0.030814  4.696582e-03  0.000100  0.100139  0.000218  1.000000   \n",
       "..        ...       ...           ...       ...       ...       ...       ...   \n",
       "543  0.100000  0.011880  1.038845e-01  0.058294  0.483309  0.271518  0.332834   \n",
       "544  0.018542  0.018771  1.807094e-01  0.000100  0.100000  0.000106  1.000000   \n",
       "545  0.099969  0.039687  9.343772e-02  0.040354  0.267501  0.112234  0.630343   \n",
       "546  0.056240  0.031443  1.643654e-07  0.000100  0.100001  0.000100  1.000000   \n",
       "547  0.100000  0.040000  1.008357e-01  0.044780  0.403570  0.174828  0.774085   \n",
       "\n",
       "               w2            w3  n_spikes  Target  \n",
       "0    4.179009e-01  2.958779e-01    1015.0     0.0  \n",
       "1    1.192093e-07  1.421085e-14   10364.0     0.0  \n",
       "2    1.192093e-07  1.421085e-14    4589.0     0.0  \n",
       "3    1.192093e-07  1.421085e-14    8274.0     0.0  \n",
       "4    1.192093e-07  1.421085e-14    4151.0     0.0  \n",
       "..            ...           ...       ...     ...  \n",
       "543  4.549999e-01  2.121657e-01    1029.0     1.0  \n",
       "544  1.192093e-07  1.421085e-14   10784.0     1.0  \n",
       "545  2.080073e-01  1.616492e-01    1523.0     1.0  \n",
       "546  1.192093e-07  1.421085e-14    3555.0     1.0  \n",
       "547  1.422010e-01  8.371432e-02    1475.0     1.0  \n",
       "\n",
       "[548 rows x 11 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    317\n",
       "1.0    231\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(['Target'],axis=1)\n",
    "y= dataset['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth':np.linspace(2,10,dtype='int'),'criterion':['gini','entropy']}\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'criterion': 'entropy', 'max_depth': 4} \n",
      "\n",
      "Classification report on Test set\n",
      "\n",
      "Accuracy:  0.912\n",
      "Recall:  0.931\n",
      "Precision:  0.871\n",
      "ROC AUC:  0.915\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':np.logspace(-3,3,50),'penalty':['l1','l2','elasticnet']}\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear',random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'C': 0.21209508879201905, 'penalty': 'l1'} \n",
      "\n",
      "Classification report on Test set\n",
      "\n",
      "Accuracy:  0.825\n",
      "Recall:  0.655\n",
      "Precision:  0.905\n",
      "ROC AUC:  0.802\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'C':np.logspace(-3,3,50),'kernel':['linear','rbf']}\n",
    "\n",
    "clf = SVC(random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found : {'C': 8.286427728546842, 'kernel': 'rbf'} \n",
      "\n",
      "Classification report on Test set\n",
      "\n",
      "Accuracy:  0.927\n",
      "Recall:  0.931\n",
      "Precision:  0.9\n",
      "ROC AUC:  0.928\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1000 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:   22.5s\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 597 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done 669 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 706 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done 745 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   33.4s\n",
      "[Parallel(n_jobs=-1)]: Done 825 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 866 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=-1)]: Done 909 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1042 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1089 tasks      | elapsed:   46.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1185 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   52.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1285 tasks      | elapsed:   54.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1336 tasks      | elapsed:   56.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1389 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1497 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1609 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1666 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1725 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1845 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1906 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2097 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2162 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2229 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2296 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2365 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2505 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2649 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2722 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2797 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2872 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2949 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3026 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3105 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3346 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3429 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3512 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3597 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3682 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3769 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3945 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':np.linspace(10,150,dtype='int',num=10),'max_depth':np.linspace(2,5,dtype='int'),'criterion':['gini','entropy']}\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1,random_state=12)\n",
    "\n",
    "gs = GridSearchCV(clf, params, scoring='roc_auc',cv=cv,verbose=10,n_jobs=-1)\n",
    "\n",
    "gs=gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set found :\",gs.best_params_,'\\n')\n",
    "print(\"Classification report on Test set\\n\")\n",
    "\n",
    "y_true, y_pred = y_test, gs.predict(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411, 10)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 7,421\n",
      "Trainable params: 7,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(\n",
    "            20, activation=\"relu\", input_shape=(X_train.shape[1],)\n",
    "        ),\n",
    "        keras.layers.Dense(100, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(50, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.5271 - accuracy: 0.7518 - fn: 48.0000 - fp: 54.0000 - tn: 184.0000 - tp: 125.0000 - precision: 0.6983 - recall: 0.7225 - val_loss: 0.4227 - val_accuracy: 0.7883 - val_fn: 28.0000 - val_fp: 1.0000 - val_tn: 78.0000 - val_tp: 30.0000 - val_precision: 0.9677 - val_recall: 0.5172\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.8345 - fn: 43.0000 - fp: 25.0000 - tn: 213.0000 - tp: 130.0000 - precision: 0.8387 - recall: 0.7514 - val_loss: 0.3456 - val_accuracy: 0.8613 - val_fn: 13.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 45.0000 - val_precision: 0.8824 - val_recall: 0.7759\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3486 - accuracy: 0.8516 - fn: 32.0000 - fp: 29.0000 - tn: 209.0000 - tp: 141.0000 - precision: 0.8294 - recall: 0.8150 - val_loss: 0.2772 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.8832 - fn: 23.0000 - fp: 25.0000 - tn: 213.0000 - tp: 150.0000 - precision: 0.8571 - recall: 0.8671 - val_loss: 0.2527 - val_accuracy: 0.8759 - val_fn: 6.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 52.0000 - val_precision: 0.8254 - val_recall: 0.8966\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3030 - accuracy: 0.8783 - fn: 25.0000 - fp: 25.0000 - tn: 213.0000 - tp: 148.0000 - precision: 0.8555 - recall: 0.8555 - val_loss: 0.2816 - val_accuracy: 0.8978 - val_fn: 9.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 49.0000 - val_precision: 0.9074 - val_recall: 0.8448\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2938 - accuracy: 0.8929 - fn: 16.0000 - fp: 28.0000 - tn: 210.0000 - tp: 157.0000 - precision: 0.8486 - recall: 0.9075 - val_loss: 0.2240 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2486 - accuracy: 0.8929 - fn: 21.0000 - fp: 23.0000 - tn: 215.0000 - tp: 152.0000 - precision: 0.8686 - recall: 0.8786 - val_loss: 0.2636 - val_accuracy: 0.8759 - val_fn: 2.0000 - val_fp: 15.0000 - val_tn: 64.0000 - val_tp: 56.0000 - val_precision: 0.7887 - val_recall: 0.9655\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2443 - accuracy: 0.8856 - fn: 22.0000 - fp: 25.0000 - tn: 213.0000 - tp: 151.0000 - precision: 0.8580 - recall: 0.8728 - val_loss: 0.1924 - val_accuracy: 0.9197 - val_fn: 6.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 52.0000 - val_precision: 0.9123 - val_recall: 0.8966\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2426 - accuracy: 0.9002 - fn: 15.0000 - fp: 26.0000 - tn: 212.0000 - tp: 158.0000 - precision: 0.8587 - recall: 0.9133 - val_loss: 0.2459 - val_accuracy: 0.9051 - val_fn: 7.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 51.0000 - val_precision: 0.8947 - val_recall: 0.8793\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2641 - accuracy: 0.8978 - fn: 19.0000 - fp: 23.0000 - tn: 215.0000 - tp: 154.0000 - precision: 0.8701 - recall: 0.8902 - val_loss: 0.2088 - val_accuracy: 0.9197 - val_fn: 4.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.9310\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2685 - accuracy: 0.8954 - fn: 17.0000 - fp: 26.0000 - tn: 212.0000 - tp: 156.0000 - precision: 0.8571 - recall: 0.9017 - val_loss: 0.2320 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2420 - accuracy: 0.8954 - fn: 22.0000 - fp: 21.0000 - tn: 217.0000 - tp: 151.0000 - precision: 0.8779 - recall: 0.8728 - val_loss: 0.2020 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2688 - accuracy: 0.8954 - fn: 16.0000 - fp: 27.0000 - tn: 211.0000 - tp: 157.0000 - precision: 0.8533 - recall: 0.9075 - val_loss: 0.1986 - val_accuracy: 0.9124 - val_fn: 7.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 51.0000 - val_precision: 0.9107 - val_recall: 0.8793\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2939 - accuracy: 0.8929 - fn: 24.0000 - fp: 20.0000 - tn: 218.0000 - tp: 149.0000 - precision: 0.8817 - recall: 0.8613 - val_loss: 0.2057 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2288 - accuracy: 0.9246 - fn: 19.0000 - fp: 12.0000 - tn: 226.0000 - tp: 154.0000 - precision: 0.9277 - recall: 0.8902 - val_loss: 0.1996 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.2461 - accuracy: 0.8929 - fn: 19.0000 - fp: 25.0000 - tn: 213.0000 - tp: 154.0000 - precision: 0.8603 - recall: 0.8902 - val_loss: 0.2016 - val_accuracy: 0.9124 - val_fn: 5.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 53.0000 - val_precision: 0.8833 - val_recall: 0.9138\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2459 - accuracy: 0.9221 - fn: 13.0000 - fp: 19.0000 - tn: 219.0000 - tp: 160.0000 - precision: 0.8939 - recall: 0.9249 - val_loss: 0.2390 - val_accuracy: 0.9270 - val_fn: 7.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 51.0000 - val_precision: 0.9444 - val_recall: 0.8793\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2537 - accuracy: 0.9148 - fn: 16.0000 - fp: 19.0000 - tn: 219.0000 - tp: 157.0000 - precision: 0.8920 - recall: 0.9075 - val_loss: 0.2367 - val_accuracy: 0.9124 - val_fn: 6.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 52.0000 - val_precision: 0.8966 - val_recall: 0.8966\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2451 - accuracy: 0.9173 - fn: 19.0000 - fp: 15.0000 - tn: 223.0000 - tp: 154.0000 - precision: 0.9112 - recall: 0.8902 - val_loss: 0.1875 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2174 - accuracy: 0.9173 - fn: 19.0000 - fp: 15.0000 - tn: 223.0000 - tp: 154.0000 - precision: 0.9112 - recall: 0.8902 - val_loss: 0.1732 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2640 - accuracy: 0.9392 - fn: 12.0000 - fp: 13.0000 - tn: 225.0000 - tp: 161.0000 - precision: 0.9253 - recall: 0.9306 - val_loss: 0.1590 - val_accuracy: 0.9197 - val_fn: 7.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 51.0000 - val_precision: 0.9273 - val_recall: 0.8793\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2327 - accuracy: 0.9173 - fn: 21.0000 - fp: 13.0000 - tn: 225.0000 - tp: 152.0000 - precision: 0.9212 - recall: 0.8786 - val_loss: 0.1573 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2057 - accuracy: 0.9100 - fn: 20.0000 - fp: 17.0000 - tn: 221.0000 - tp: 153.0000 - precision: 0.9000 - recall: 0.8844 - val_loss: 0.1685 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2468 - accuracy: 0.9100 - fn: 17.0000 - fp: 20.0000 - tn: 218.0000 - tp: 156.0000 - precision: 0.8864 - recall: 0.9017 - val_loss: 0.1950 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2381 - accuracy: 0.8905 - fn: 28.0000 - fp: 17.0000 - tn: 221.0000 - tp: 145.0000 - precision: 0.8951 - recall: 0.8382 - val_loss: 0.2346 - val_accuracy: 0.9051 - val_fn: 3.0000 - val_fp: 10.0000 - val_tn: 69.0000 - val_tp: 55.0000 - val_precision: 0.8462 - val_recall: 0.9483\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9100 - fn: 15.0000 - fp: 22.0000 - tn: 216.0000 - tp: 158.0000 - precision: 0.8778 - recall: 0.9133 - val_loss: 0.2294 - val_accuracy: 0.8978 - val_fn: 2.0000 - val_fp: 12.0000 - val_tn: 67.0000 - val_tp: 56.0000 - val_precision: 0.8235 - val_recall: 0.9655\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9392 - fn: 11.0000 - fp: 14.0000 - tn: 224.0000 - tp: 162.0000 - precision: 0.9205 - recall: 0.9364 - val_loss: 0.1915 - val_accuracy: 0.9270 - val_fn: 5.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 53.0000 - val_precision: 0.9138 - val_recall: 0.9138\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8905 - fn: 27.0000 - fp: 18.0000 - tn: 220.0000 - tp: 146.0000 - precision: 0.8902 - recall: 0.8439 - val_loss: 0.1840 - val_accuracy: 0.9124 - val_fn: 7.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 51.0000 - val_precision: 0.9107 - val_recall: 0.8793\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2036 - accuracy: 0.9173 - fn: 18.0000 - fp: 16.0000 - tn: 222.0000 - tp: 155.0000 - precision: 0.9064 - recall: 0.8960 - val_loss: 0.1530 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2017 - accuracy: 0.9246 - fn: 13.0000 - fp: 18.0000 - tn: 220.0000 - tp: 160.0000 - precision: 0.8989 - recall: 0.9249 - val_loss: 0.2108 - val_accuracy: 0.9197 - val_fn: 1.0000 - val_fp: 10.0000 - val_tn: 69.0000 - val_tp: 57.0000 - val_precision: 0.8507 - val_recall: 0.9828\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2607 - accuracy: 0.8929 - fn: 17.0000 - fp: 27.0000 - tn: 211.0000 - tp: 156.0000 - precision: 0.8525 - recall: 0.9017 - val_loss: 0.1937 - val_accuracy: 0.9343 - val_fn: 2.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.9655\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9246 - fn: 15.0000 - fp: 16.0000 - tn: 222.0000 - tp: 158.0000 - precision: 0.9080 - recall: 0.9133 - val_loss: 0.1987 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2261 - accuracy: 0.9294 - fn: 14.0000 - fp: 15.0000 - tn: 223.0000 - tp: 159.0000 - precision: 0.9138 - recall: 0.9191 - val_loss: 0.1840 - val_accuracy: 0.9343 - val_fn: 2.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.9655\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2027 - accuracy: 0.9173 - fn: 18.0000 - fp: 16.0000 - tn: 222.0000 - tp: 155.0000 - precision: 0.9064 - recall: 0.8960 - val_loss: 0.1876 - val_accuracy: 0.9124 - val_fn: 1.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 57.0000 - val_precision: 0.8382 - val_recall: 0.9828\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2236 - accuracy: 0.9124 - fn: 19.0000 - fp: 17.0000 - tn: 221.0000 - tp: 154.0000 - precision: 0.9006 - recall: 0.8902 - val_loss: 0.1499 - val_accuracy: 0.9343 - val_fn: 2.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.9655\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1768 - accuracy: 0.9270 - fn: 18.0000 - fp: 12.0000 - tn: 226.0000 - tp: 155.0000 - precision: 0.9281 - recall: 0.8960 - val_loss: 0.1686 - val_accuracy: 0.9343 - val_fn: 2.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.9655\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9075 - fn: 19.0000 - fp: 19.0000 - tn: 219.0000 - tp: 154.0000 - precision: 0.8902 - recall: 0.8902 - val_loss: 0.1747 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1885 - accuracy: 0.9294 - fn: 14.0000 - fp: 15.0000 - tn: 223.0000 - tp: 159.0000 - precision: 0.9138 - recall: 0.9191 - val_loss: 0.2165 - val_accuracy: 0.9051 - val_fn: 2.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 56.0000 - val_precision: 0.8358 - val_recall: 0.9655\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1970 - accuracy: 0.9294 - fn: 15.0000 - fp: 14.0000 - tn: 224.0000 - tp: 158.0000 - precision: 0.9186 - recall: 0.9133 - val_loss: 0.1907 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1924 - accuracy: 0.9392 - fn: 12.0000 - fp: 13.0000 - tn: 225.0000 - tp: 161.0000 - precision: 0.9253 - recall: 0.9306 - val_loss: 0.2236 - val_accuracy: 0.9489 - val_fn: 6.0000 - val_fp: 1.0000 - val_tn: 78.0000 - val_tp: 52.0000 - val_precision: 0.9811 - val_recall: 0.8966\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9075 - fn: 19.0000 - fp: 19.0000 - tn: 219.0000 - tp: 154.0000 - precision: 0.8902 - recall: 0.8902 - val_loss: 0.2366 - val_accuracy: 0.8978 - val_fn: 2.0000 - val_fp: 12.0000 - val_tn: 67.0000 - val_tp: 56.0000 - val_precision: 0.8235 - val_recall: 0.9655\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9148 - fn: 14.0000 - fp: 21.0000 - tn: 217.0000 - tp: 159.0000 - precision: 0.8833 - recall: 0.9191 - val_loss: 0.2161 - val_accuracy: 0.8832 - val_fn: 5.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 53.0000 - val_precision: 0.8281 - val_recall: 0.9138\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2274 - accuracy: 0.9124 - fn: 17.0000 - fp: 19.0000 - tn: 219.0000 - tp: 156.0000 - precision: 0.8914 - recall: 0.9017 - val_loss: 0.1631 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9221 - fn: 19.0000 - fp: 13.0000 - tn: 225.0000 - tp: 154.0000 - precision: 0.9222 - recall: 0.8902 - val_loss: 0.1889 - val_accuracy: 0.9489 - val_fn: 4.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 54.0000 - val_precision: 0.9474 - val_recall: 0.9310\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1598 - accuracy: 0.9319 - fn: 11.0000 - fp: 17.0000 - tn: 221.0000 - tp: 162.0000 - precision: 0.9050 - recall: 0.9364 - val_loss: 0.1685 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9221 - fn: 14.0000 - fp: 18.0000 - tn: 220.0000 - tp: 159.0000 - precision: 0.8983 - recall: 0.9191 - val_loss: 0.1575 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1859 - accuracy: 0.9270 - fn: 11.0000 - fp: 19.0000 - tn: 219.0000 - tp: 162.0000 - precision: 0.8950 - recall: 0.9364 - val_loss: 0.1929 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9246 - fn: 16.0000 - fp: 15.0000 - tn: 223.0000 - tp: 157.0000 - precision: 0.9128 - recall: 0.9075 - val_loss: 0.1669 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9221 - fn: 14.0000 - fp: 18.0000 - tn: 220.0000 - tp: 159.0000 - precision: 0.8983 - recall: 0.9191 - val_loss: 0.2402 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2873 - accuracy: 0.9124 - fn: 22.0000 - fp: 14.0000 - tn: 224.0000 - tp: 151.0000 - precision: 0.9152 - recall: 0.8728 - val_loss: 0.2490 - val_accuracy: 0.8978 - val_fn: 10.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 48.0000 - val_precision: 0.9231 - val_recall: 0.8276\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2321 - accuracy: 0.9270 - fn: 19.0000 - fp: 11.0000 - tn: 227.0000 - tp: 154.0000 - precision: 0.9333 - recall: 0.8902 - val_loss: 0.1877 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.2021 - accuracy: 0.9148 - fn: 17.0000 - fp: 18.0000 - tn: 220.0000 - tp: 156.0000 - precision: 0.8966 - recall: 0.9017 - val_loss: 0.1738 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2235 - accuracy: 0.9148 - fn: 18.0000 - fp: 17.0000 - tn: 221.0000 - tp: 155.0000 - precision: 0.9012 - recall: 0.8960 - val_loss: 0.1834 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9270 - fn: 15.0000 - fp: 15.0000 - tn: 223.0000 - tp: 158.0000 - precision: 0.9133 - recall: 0.9133 - val_loss: 0.2635 - val_accuracy: 0.8905 - val_fn: 4.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 54.0000 - val_precision: 0.8308 - val_recall: 0.9310\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.2577 - accuracy: 0.9051 - fn: 15.0000 - fp: 24.0000 - tn: 214.0000 - tp: 158.0000 - precision: 0.8681 - recall: 0.9133 - val_loss: 0.1662 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9100 - fn: 15.0000 - fp: 22.0000 - tn: 216.0000 - tp: 158.0000 - precision: 0.8778 - recall: 0.9133 - val_loss: 0.2367 - val_accuracy: 0.9197 - val_fn: 2.0000 - val_fp: 9.0000 - val_tn: 70.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.9655\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1730 - accuracy: 0.9246 - fn: 21.0000 - fp: 10.0000 - tn: 228.0000 - tp: 152.0000 - precision: 0.9383 - recall: 0.8786 - val_loss: 0.1921 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.9197 - fn: 15.0000 - fp: 18.0000 - tn: 220.0000 - tp: 158.0000 - precision: 0.8977 - recall: 0.9133 - val_loss: 0.2621 - val_accuracy: 0.9270 - val_fn: 6.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 52.0000 - val_precision: 0.9286 - val_recall: 0.8966\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1800 - accuracy: 0.9319 - fn: 12.0000 - fp: 16.0000 - tn: 222.0000 - tp: 161.0000 - precision: 0.9096 - recall: 0.9306 - val_loss: 0.1763 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9221 - fn: 18.0000 - fp: 14.0000 - tn: 224.0000 - tp: 155.0000 - precision: 0.9172 - recall: 0.8960 - val_loss: 0.1564 - val_accuracy: 0.9489 - val_fn: 1.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 57.0000 - val_precision: 0.9048 - val_recall: 0.9828\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9027 - fn: 17.0000 - fp: 23.0000 - tn: 215.0000 - tp: 156.0000 - precision: 0.8715 - recall: 0.9017 - val_loss: 0.1939 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2035 - accuracy: 0.9148 - fn: 20.0000 - fp: 15.0000 - tn: 223.0000 - tp: 153.0000 - precision: 0.9107 - recall: 0.8844 - val_loss: 0.1925 - val_accuracy: 0.9562 - val_fn: 4.0000 - val_fp: 2.0000 - val_tn: 77.0000 - val_tp: 54.0000 - val_precision: 0.9643 - val_recall: 0.9310\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9148 - fn: 16.0000 - fp: 19.0000 - tn: 219.0000 - tp: 157.0000 - precision: 0.8920 - recall: 0.9075 - val_loss: 0.2323 - val_accuracy: 0.9416 - val_fn: 6.0000 - val_fp: 2.0000 - val_tn: 77.0000 - val_tp: 52.0000 - val_precision: 0.9630 - val_recall: 0.8966\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9270 - fn: 12.0000 - fp: 18.0000 - tn: 220.0000 - tp: 161.0000 - precision: 0.8994 - recall: 0.9306 - val_loss: 0.2267 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9367 - fn: 14.0000 - fp: 12.0000 - tn: 226.0000 - tp: 159.0000 - precision: 0.9298 - recall: 0.9191 - val_loss: 0.2565 - val_accuracy: 0.9270 - val_fn: 8.0000 - val_fp: 2.0000 - val_tn: 77.0000 - val_tp: 50.0000 - val_precision: 0.9615 - val_recall: 0.8621\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9027 - fn: 19.0000 - fp: 21.0000 - tn: 217.0000 - tp: 154.0000 - precision: 0.8800 - recall: 0.8902 - val_loss: 0.3199 - val_accuracy: 0.8978 - val_fn: 3.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 55.0000 - val_precision: 0.8333 - val_recall: 0.9483\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2317 - accuracy: 0.9051 - fn: 14.0000 - fp: 25.0000 - tn: 213.0000 - tp: 159.0000 - precision: 0.8641 - recall: 0.9191 - val_loss: 0.1905 - val_accuracy: 0.9416 - val_fn: 5.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 53.0000 - val_precision: 0.9464 - val_recall: 0.9138\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9367 - fn: 16.0000 - fp: 10.0000 - tn: 228.0000 - tp: 157.0000 - precision: 0.9401 - recall: 0.9075 - val_loss: 0.1848 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9319 - fn: 17.0000 - fp: 11.0000 - tn: 227.0000 - tp: 156.0000 - precision: 0.9341 - recall: 0.9017 - val_loss: 0.2340 - val_accuracy: 0.9416 - val_fn: 5.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 53.0000 - val_precision: 0.9464 - val_recall: 0.9138\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1347 - accuracy: 0.9343 - fn: 13.0000 - fp: 14.0000 - tn: 224.0000 - tp: 160.0000 - precision: 0.9195 - recall: 0.9249 - val_loss: 0.2183 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1750 - accuracy: 0.9367 - fn: 15.0000 - fp: 11.0000 - tn: 227.0000 - tp: 158.0000 - precision: 0.9349 - recall: 0.9133 - val_loss: 0.1595 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9319 - fn: 14.0000 - fp: 14.0000 - tn: 224.0000 - tp: 159.0000 - precision: 0.9191 - recall: 0.9191 - val_loss: 0.3618 - val_accuracy: 0.9197 - val_fn: 4.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 54.0000 - val_precision: 0.8852 - val_recall: 0.9310\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1473 - accuracy: 0.9343 - fn: 12.0000 - fp: 15.0000 - tn: 223.0000 - tp: 161.0000 - precision: 0.9148 - recall: 0.9306 - val_loss: 0.3126 - val_accuracy: 0.9051 - val_fn: 7.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 51.0000 - val_precision: 0.8947 - val_recall: 0.8793\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9392 - fn: 15.0000 - fp: 10.0000 - tn: 228.0000 - tp: 158.0000 - precision: 0.9405 - recall: 0.9133 - val_loss: 0.2174 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9465 - fn: 14.0000 - fp: 8.0000 - tn: 230.0000 - tp: 159.0000 - precision: 0.9521 - recall: 0.9191 - val_loss: 0.2050 - val_accuracy: 0.9343 - val_fn: 5.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 53.0000 - val_precision: 0.9298 - val_recall: 0.9138\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1476 - accuracy: 0.9367 - fn: 11.0000 - fp: 15.0000 - tn: 223.0000 - tp: 162.0000 - precision: 0.9153 - recall: 0.9364 - val_loss: 0.2347 - val_accuracy: 0.9343 - val_fn: 6.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 52.0000 - val_precision: 0.9455 - val_recall: 0.8966\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9416 - fn: 13.0000 - fp: 11.0000 - tn: 227.0000 - tp: 160.0000 - precision: 0.9357 - recall: 0.9249 - val_loss: 0.2446 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1459 - accuracy: 0.9367 - fn: 17.0000 - fp: 9.0000 - tn: 229.0000 - tp: 156.0000 - precision: 0.9455 - recall: 0.9017 - val_loss: 0.3007 - val_accuracy: 0.9343 - val_fn: 1.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.9828\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.4516 - accuracy: 0.9002 - fn: 17.0000 - fp: 24.0000 - tn: 214.0000 - tp: 156.0000 - precision: 0.8667 - recall: 0.9017 - val_loss: 0.2661 - val_accuracy: 0.8832 - val_fn: 4.0000 - val_fp: 12.0000 - val_tn: 67.0000 - val_tp: 54.0000 - val_precision: 0.8182 - val_recall: 0.9310\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.3332 - accuracy: 0.8735 - fn: 30.0000 - fp: 22.0000 - tn: 216.0000 - tp: 143.0000 - precision: 0.8667 - recall: 0.8266 - val_loss: 0.2428 - val_accuracy: 0.9197 - val_fn: 2.0000 - val_fp: 9.0000 - val_tn: 70.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.9655\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2428 - accuracy: 0.9027 - fn: 20.0000 - fp: 20.0000 - tn: 218.0000 - tp: 153.0000 - precision: 0.8844 - recall: 0.8844 - val_loss: 0.2104 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2258 - accuracy: 0.9027 - fn: 13.0000 - fp: 27.0000 - tn: 211.0000 - tp: 160.0000 - precision: 0.8556 - recall: 0.9249 - val_loss: 0.2232 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2155 - accuracy: 0.9221 - fn: 9.0000 - fp: 23.0000 - tn: 215.0000 - tp: 164.0000 - precision: 0.8770 - recall: 0.9480 - val_loss: 0.2064 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2076 - accuracy: 0.9197 - fn: 8.0000 - fp: 25.0000 - tn: 213.0000 - tp: 165.0000 - precision: 0.8684 - recall: 0.9538 - val_loss: 0.3877 - val_accuracy: 0.9343 - val_fn: 1.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 57.0000 - val_precision: 0.8769 - val_recall: 0.9828\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2727 - accuracy: 0.8978 - fn: 16.0000 - fp: 26.0000 - tn: 212.0000 - tp: 157.0000 - precision: 0.8579 - recall: 0.9075 - val_loss: 0.2220 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.2238 - accuracy: 0.9173 - fn: 12.0000 - fp: 22.0000 - tn: 216.0000 - tp: 161.0000 - precision: 0.8798 - recall: 0.9306 - val_loss: 0.1866 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1872 - accuracy: 0.9221 - fn: 14.0000 - fp: 18.0000 - tn: 220.0000 - tp: 159.0000 - precision: 0.8983 - recall: 0.9191 - val_loss: 0.1894 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1834 - accuracy: 0.9319 - fn: 14.0000 - fp: 14.0000 - tn: 224.0000 - tp: 159.0000 - precision: 0.9191 - recall: 0.9191 - val_loss: 0.1770 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9343 - fn: 13.0000 - fp: 14.0000 - tn: 224.0000 - tp: 160.0000 - precision: 0.9195 - recall: 0.9249 - val_loss: 0.1826 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.2038 - accuracy: 0.9002 - fn: 15.0000 - fp: 26.0000 - tn: 212.0000 - tp: 158.0000 - precision: 0.8587 - recall: 0.9133 - val_loss: 0.2192 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9124 - fn: 16.0000 - fp: 20.0000 - tn: 218.0000 - tp: 157.0000 - precision: 0.8870 - recall: 0.9075 - val_loss: 0.2210 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1645 - accuracy: 0.9343 - fn: 7.0000 - fp: 20.0000 - tn: 218.0000 - tp: 166.0000 - precision: 0.8925 - recall: 0.9595 - val_loss: 0.2210 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1672 - accuracy: 0.9221 - fn: 11.0000 - fp: 21.0000 - tn: 217.0000 - tp: 162.0000 - precision: 0.8852 - recall: 0.9364 - val_loss: 0.2586 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1774 - accuracy: 0.9197 - fn: 15.0000 - fp: 18.0000 - tn: 220.0000 - tp: 158.0000 - precision: 0.8977 - recall: 0.9133 - val_loss: 0.2528 - val_accuracy: 0.9343 - val_fn: 2.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 56.0000 - val_precision: 0.8889 - val_recall: 0.9655\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9319 - fn: 9.0000 - fp: 19.0000 - tn: 219.0000 - tp: 164.0000 - precision: 0.8962 - recall: 0.9480 - val_loss: 0.3056 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.9246 - fn: 13.0000 - fp: 18.0000 - tn: 220.0000 - tp: 160.0000 - precision: 0.8989 - recall: 0.9249 - val_loss: 0.3034 - val_accuracy: 0.9270 - val_fn: 4.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 54.0000 - val_precision: 0.9000 - val_recall: 0.9310\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1911 - accuracy: 0.9246 - fn: 12.0000 - fp: 19.0000 - tn: 219.0000 - tp: 161.0000 - precision: 0.8944 - recall: 0.9306 - val_loss: 0.2012 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2057 - accuracy: 0.9343 - fn: 13.0000 - fp: 14.0000 - tn: 224.0000 - tp: 160.0000 - precision: 0.9195 - recall: 0.9249 - val_loss: 0.1706 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9367 - fn: 10.0000 - fp: 16.0000 - tn: 222.0000 - tp: 163.0000 - precision: 0.9106 - recall: 0.9422 - val_loss: 0.2036 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1533 - accuracy: 0.9440 - fn: 7.0000 - fp: 16.0000 - tn: 222.0000 - tp: 166.0000 - precision: 0.9121 - recall: 0.9595 - val_loss: 0.2837 - val_accuracy: 0.9270 - val_fn: 5.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 53.0000 - val_precision: 0.9138 - val_recall: 0.9138\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9319 - fn: 11.0000 - fp: 17.0000 - tn: 221.0000 - tp: 162.0000 - precision: 0.9050 - recall: 0.9364 - val_loss: 0.1967 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1524 - accuracy: 0.9148 - fn: 19.0000 - fp: 16.0000 - tn: 222.0000 - tp: 154.0000 - precision: 0.9059 - recall: 0.8902 - val_loss: 0.2731 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1638 - accuracy: 0.9440 - fn: 9.0000 - fp: 14.0000 - tn: 224.0000 - tp: 164.0000 - precision: 0.9213 - recall: 0.9480 - val_loss: 0.2204 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9538 - fn: 11.0000 - fp: 8.0000 - tn: 230.0000 - tp: 162.0000 - precision: 0.9529 - recall: 0.9364 - val_loss: 0.2212 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9367 - fn: 8.0000 - fp: 18.0000 - tn: 220.0000 - tp: 165.0000 - precision: 0.9016 - recall: 0.9538 - val_loss: 0.2476 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1618 - accuracy: 0.9270 - fn: 17.0000 - fp: 13.0000 - tn: 225.0000 - tp: 156.0000 - precision: 0.9231 - recall: 0.9017 - val_loss: 0.2804 - val_accuracy: 0.9051 - val_fn: 8.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 50.0000 - val_precision: 0.9091 - val_recall: 0.8621\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9124 - fn: 22.0000 - fp: 14.0000 - tn: 224.0000 - tp: 151.0000 - precision: 0.9152 - recall: 0.8728 - val_loss: 0.3889 - val_accuracy: 0.9270 - val_fn: 2.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.9655\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.9294 - fn: 12.0000 - fp: 17.0000 - tn: 221.0000 - tp: 161.0000 - precision: 0.9045 - recall: 0.9306 - val_loss: 0.2608 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9319 - fn: 12.0000 - fp: 16.0000 - tn: 222.0000 - tp: 161.0000 - precision: 0.9096 - recall: 0.9306 - val_loss: 0.1900 - val_accuracy: 0.9270 - val_fn: 2.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.9655\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.9246 - fn: 10.0000 - fp: 21.0000 - tn: 217.0000 - tp: 163.0000 - precision: 0.8859 - recall: 0.9422 - val_loss: 0.2072 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9343 - fn: 14.0000 - fp: 13.0000 - tn: 225.0000 - tp: 159.0000 - precision: 0.9244 - recall: 0.9191 - val_loss: 0.1747 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9294 - fn: 13.0000 - fp: 16.0000 - tn: 222.0000 - tp: 160.0000 - precision: 0.9091 - recall: 0.9249 - val_loss: 0.2358 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.3118 - accuracy: 0.9100 - fn: 19.0000 - fp: 18.0000 - tn: 220.0000 - tp: 154.0000 - precision: 0.8953 - recall: 0.8902 - val_loss: 0.2363 - val_accuracy: 0.8832 - val_fn: 2.0000 - val_fp: 14.0000 - val_tn: 65.0000 - val_tp: 56.0000 - val_precision: 0.8000 - val_recall: 0.9655\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9270 - fn: 9.0000 - fp: 21.0000 - tn: 217.0000 - tp: 164.0000 - precision: 0.8865 - recall: 0.9480 - val_loss: 0.2129 - val_accuracy: 0.9343 - val_fn: 3.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 55.0000 - val_precision: 0.9016 - val_recall: 0.9483\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1621 - accuracy: 0.9221 - fn: 13.0000 - fp: 19.0000 - tn: 219.0000 - tp: 160.0000 - precision: 0.8939 - recall: 0.9249 - val_loss: 0.1613 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1553 - accuracy: 0.9270 - fn: 14.0000 - fp: 16.0000 - tn: 222.0000 - tp: 159.0000 - precision: 0.9086 - recall: 0.9191 - val_loss: 0.2117 - val_accuracy: 0.9343 - val_fn: 4.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 54.0000 - val_precision: 0.9153 - val_recall: 0.9310\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9392 - fn: 14.0000 - fp: 11.0000 - tn: 227.0000 - tp: 159.0000 - precision: 0.9353 - recall: 0.9191 - val_loss: 0.1939 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1408 - accuracy: 0.9513 - fn: 11.0000 - fp: 9.0000 - tn: 229.0000 - tp: 162.0000 - precision: 0.9474 - recall: 0.9364 - val_loss: 0.3066 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1993 - accuracy: 0.9343 - fn: 16.0000 - fp: 11.0000 - tn: 227.0000 - tp: 157.0000 - precision: 0.9345 - recall: 0.9075 - val_loss: 0.1583 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1518 - accuracy: 0.9197 - fn: 17.0000 - fp: 16.0000 - tn: 222.0000 - tp: 156.0000 - precision: 0.9070 - recall: 0.9017 - val_loss: 0.1992 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1458 - accuracy: 0.9416 - fn: 10.0000 - fp: 14.0000 - tn: 224.0000 - tp: 163.0000 - precision: 0.9209 - recall: 0.9422 - val_loss: 0.1701 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9465 - fn: 11.0000 - fp: 11.0000 - tn: 227.0000 - tp: 162.0000 - precision: 0.9364 - recall: 0.9364 - val_loss: 0.1837 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9416 - fn: 14.0000 - fp: 10.0000 - tn: 228.0000 - tp: 159.0000 - precision: 0.9408 - recall: 0.9191 - val_loss: 0.2532 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9367 - fn: 10.0000 - fp: 16.0000 - tn: 222.0000 - tp: 163.0000 - precision: 0.9106 - recall: 0.9422 - val_loss: 0.2190 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1694 - accuracy: 0.9416 - fn: 11.0000 - fp: 13.0000 - tn: 225.0000 - tp: 162.0000 - precision: 0.9257 - recall: 0.9364 - val_loss: 0.2207 - val_accuracy: 0.9197 - val_fn: 5.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 53.0000 - val_precision: 0.8983 - val_recall: 0.9138\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.9538 - fn: 12.0000 - fp: 7.0000 - tn: 231.0000 - tp: 161.0000 - precision: 0.9583 - recall: 0.9306 - val_loss: 0.1857 - val_accuracy: 0.9635 - val_fn: 2.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 56.0000 - val_precision: 0.9492 - val_recall: 0.9655\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.9367 - fn: 17.0000 - fp: 9.0000 - tn: 229.0000 - tp: 156.0000 - precision: 0.9455 - recall: 0.9017 - val_loss: 0.2534 - val_accuracy: 0.9051 - val_fn: 2.0000 - val_fp: 11.0000 - val_tn: 68.0000 - val_tp: 56.0000 - val_precision: 0.8358 - val_recall: 0.9655\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1780 - accuracy: 0.9246 - fn: 16.0000 - fp: 15.0000 - tn: 223.0000 - tp: 157.0000 - precision: 0.9128 - recall: 0.9075 - val_loss: 0.2010 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1553 - accuracy: 0.9465 - fn: 14.0000 - fp: 8.0000 - tn: 230.0000 - tp: 159.0000 - precision: 0.9521 - recall: 0.9191 - val_loss: 0.1698 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9465 - fn: 9.0000 - fp: 13.0000 - tn: 225.0000 - tp: 164.0000 - precision: 0.9266 - recall: 0.9480 - val_loss: 0.2301 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1556 - accuracy: 0.9319 - fn: 15.0000 - fp: 13.0000 - tn: 225.0000 - tp: 158.0000 - precision: 0.9240 - recall: 0.9133 - val_loss: 0.1440 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9538 - fn: 11.0000 - fp: 8.0000 - tn: 230.0000 - tp: 162.0000 - precision: 0.9529 - recall: 0.9364 - val_loss: 0.2089 - val_accuracy: 0.9562 - val_fn: 3.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 55.0000 - val_precision: 0.9483 - val_recall: 0.9483\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1311 - accuracy: 0.9489 - fn: 11.0000 - fp: 10.0000 - tn: 228.0000 - tp: 162.0000 - precision: 0.9419 - recall: 0.9364 - val_loss: 0.2861 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9319 - fn: 11.0000 - fp: 17.0000 - tn: 221.0000 - tp: 162.0000 - precision: 0.9050 - recall: 0.9364 - val_loss: 0.2148 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.9392 - fn: 13.0000 - fp: 12.0000 - tn: 226.0000 - tp: 160.0000 - precision: 0.9302 - recall: 0.9249 - val_loss: 0.1651 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1474 - accuracy: 0.9367 - fn: 12.0000 - fp: 14.0000 - tn: 224.0000 - tp: 161.0000 - precision: 0.9200 - recall: 0.9306 - val_loss: 0.1495 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1565 - accuracy: 0.9416 - fn: 15.0000 - fp: 9.0000 - tn: 229.0000 - tp: 158.0000 - precision: 0.9461 - recall: 0.9133 - val_loss: 0.2070 - val_accuracy: 0.9562 - val_fn: 3.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 55.0000 - val_precision: 0.9483 - val_recall: 0.9483\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1523 - accuracy: 0.9416 - fn: 13.0000 - fp: 11.0000 - tn: 227.0000 - tp: 160.0000 - precision: 0.9357 - recall: 0.9249 - val_loss: 0.1306 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2034 - accuracy: 0.8954 - fn: 12.0000 - fp: 31.0000 - tn: 207.0000 - tp: 161.0000 - precision: 0.8385 - recall: 0.9306 - val_loss: 0.2961 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9416 - fn: 11.0000 - fp: 13.0000 - tn: 225.0000 - tp: 162.0000 - precision: 0.9257 - recall: 0.9364 - val_loss: 0.2162 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1790 - accuracy: 0.9416 - fn: 11.0000 - fp: 13.0000 - tn: 225.0000 - tp: 162.0000 - precision: 0.9257 - recall: 0.9364 - val_loss: 0.4610 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9343 - fn: 14.0000 - fp: 13.0000 - tn: 225.0000 - tp: 159.0000 - precision: 0.9244 - recall: 0.9191 - val_loss: 0.1857 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9440 - fn: 12.0000 - fp: 11.0000 - tn: 227.0000 - tp: 161.0000 - precision: 0.9360 - recall: 0.9306 - val_loss: 0.2372 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9416 - fn: 9.0000 - fp: 15.0000 - tn: 223.0000 - tp: 164.0000 - precision: 0.9162 - recall: 0.9480 - val_loss: 0.1491 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9270 - fn: 14.0000 - fp: 16.0000 - tn: 222.0000 - tp: 159.0000 - precision: 0.9086 - recall: 0.9191 - val_loss: 0.2057 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.9416 - fn: 15.0000 - fp: 9.0000 - tn: 229.0000 - tp: 158.0000 - precision: 0.9461 - recall: 0.9133 - val_loss: 0.2573 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.9513 - fn: 9.0000 - fp: 11.0000 - tn: 227.0000 - tp: 164.0000 - precision: 0.9371 - recall: 0.9480 - val_loss: 0.2552 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1703 - accuracy: 0.9489 - fn: 11.0000 - fp: 10.0000 - tn: 228.0000 - tp: 162.0000 - precision: 0.9419 - recall: 0.9364 - val_loss: 0.2023 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9465 - fn: 12.0000 - fp: 10.0000 - tn: 228.0000 - tp: 161.0000 - precision: 0.9415 - recall: 0.9306 - val_loss: 0.2317 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1543 - accuracy: 0.9343 - fn: 11.0000 - fp: 16.0000 - tn: 222.0000 - tp: 162.0000 - precision: 0.9101 - recall: 0.9364 - val_loss: 0.2706 - val_accuracy: 0.8978 - val_fn: 4.0000 - val_fp: 10.0000 - val_tn: 69.0000 - val_tp: 54.0000 - val_precision: 0.8438 - val_recall: 0.9310\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1437 - accuracy: 0.9440 - fn: 12.0000 - fp: 11.0000 - tn: 227.0000 - tp: 161.0000 - precision: 0.9360 - recall: 0.9306 - val_loss: 0.2410 - val_accuracy: 0.9124 - val_fn: 4.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 54.0000 - val_precision: 0.8710 - val_recall: 0.9310\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9416 - fn: 17.0000 - fp: 7.0000 - tn: 231.0000 - tp: 156.0000 - precision: 0.9571 - recall: 0.9017 - val_loss: 0.2383 - val_accuracy: 0.9489 - val_fn: 1.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 57.0000 - val_precision: 0.9048 - val_recall: 0.9828\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9513 - fn: 12.0000 - fp: 8.0000 - tn: 230.0000 - tp: 161.0000 - precision: 0.9527 - recall: 0.9306 - val_loss: 0.2973 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9440 - fn: 12.0000 - fp: 11.0000 - tn: 227.0000 - tp: 161.0000 - precision: 0.9360 - recall: 0.9306 - val_loss: 0.2558 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.9513 - fn: 11.0000 - fp: 9.0000 - tn: 229.0000 - tp: 162.0000 - precision: 0.9474 - recall: 0.9364 - val_loss: 0.2832 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9465 - fn: 14.0000 - fp: 8.0000 - tn: 230.0000 - tp: 159.0000 - precision: 0.9521 - recall: 0.9191 - val_loss: 0.2777 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1393 - accuracy: 0.9392 - fn: 15.0000 - fp: 10.0000 - tn: 228.0000 - tp: 158.0000 - precision: 0.9405 - recall: 0.9133 - val_loss: 0.2130 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9489 - fn: 14.0000 - fp: 7.0000 - tn: 231.0000 - tp: 159.0000 - precision: 0.9578 - recall: 0.9191 - val_loss: 0.2798 - val_accuracy: 0.9416 - val_fn: 1.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 57.0000 - val_precision: 0.8906 - val_recall: 0.9828\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9416 - fn: 13.0000 - fp: 11.0000 - tn: 227.0000 - tp: 160.0000 - precision: 0.9357 - recall: 0.9249 - val_loss: 0.2524 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9416 - fn: 16.0000 - fp: 8.0000 - tn: 230.0000 - tp: 157.0000 - precision: 0.9515 - recall: 0.9075 - val_loss: 0.3017 - val_accuracy: 0.9489 - val_fn: 1.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 57.0000 - val_precision: 0.9048 - val_recall: 0.9828\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9465 - fn: 9.0000 - fp: 13.0000 - tn: 225.0000 - tp: 164.0000 - precision: 0.9266 - recall: 0.9480 - val_loss: 0.2057 - val_accuracy: 0.9562 - val_fn: 3.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 55.0000 - val_precision: 0.9483 - val_recall: 0.9483\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9513 - fn: 10.0000 - fp: 10.0000 - tn: 228.0000 - tp: 163.0000 - precision: 0.9422 - recall: 0.9422 - val_loss: 0.2220 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9538 - fn: 12.0000 - fp: 7.0000 - tn: 231.0000 - tp: 161.0000 - precision: 0.9583 - recall: 0.9306 - val_loss: 0.2303 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9586 - fn: 11.0000 - fp: 6.0000 - tn: 232.0000 - tp: 162.0000 - precision: 0.9643 - recall: 0.9364 - val_loss: 0.2819 - val_accuracy: 0.9489 - val_fn: 2.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 56.0000 - val_precision: 0.9180 - val_recall: 0.9655\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9319 - fn: 13.0000 - fp: 15.0000 - tn: 223.0000 - tp: 160.0000 - precision: 0.9143 - recall: 0.9249 - val_loss: 0.1973 - val_accuracy: 0.9270 - val_fn: 2.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.9655\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9489 - fn: 10.0000 - fp: 11.0000 - tn: 227.0000 - tp: 163.0000 - precision: 0.9368 - recall: 0.9422 - val_loss: 0.2500 - val_accuracy: 0.9416 - val_fn: 2.0000 - val_fp: 6.0000 - val_tn: 73.0000 - val_tp: 56.0000 - val_precision: 0.9032 - val_recall: 0.9655\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9465 - fn: 11.0000 - fp: 11.0000 - tn: 227.0000 - tp: 162.0000 - precision: 0.9364 - recall: 0.9364 - val_loss: 0.2683 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9513 - fn: 11.0000 - fp: 9.0000 - tn: 229.0000 - tp: 162.0000 - precision: 0.9474 - recall: 0.9364 - val_loss: 0.2771 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1627 - accuracy: 0.9489 - fn: 13.0000 - fp: 8.0000 - tn: 230.0000 - tp: 160.0000 - precision: 0.9524 - recall: 0.9249 - val_loss: 0.3223 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2150 - accuracy: 0.9270 - fn: 13.0000 - fp: 17.0000 - tn: 221.0000 - tp: 160.0000 - precision: 0.9040 - recall: 0.9249 - val_loss: 0.3067 - val_accuracy: 0.9270 - val_fn: 5.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 53.0000 - val_precision: 0.9138 - val_recall: 0.9138\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9513 - fn: 11.0000 - fp: 9.0000 - tn: 229.0000 - tp: 162.0000 - precision: 0.9474 - recall: 0.9364 - val_loss: 0.2524 - val_accuracy: 0.9124 - val_fn: 8.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 50.0000 - val_precision: 0.9259 - val_recall: 0.8621\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9100 - fn: 25.0000 - fp: 12.0000 - tn: 226.0000 - tp: 148.0000 - precision: 0.9250 - recall: 0.8555 - val_loss: 0.2703 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9392 - fn: 14.0000 - fp: 11.0000 - tn: 227.0000 - tp: 159.0000 - precision: 0.9353 - recall: 0.9191 - val_loss: 0.2543 - val_accuracy: 0.9562 - val_fn: 3.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 55.0000 - val_precision: 0.9483 - val_recall: 0.9483\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9367 - fn: 14.0000 - fp: 12.0000 - tn: 226.0000 - tp: 159.0000 - precision: 0.9298 - recall: 0.9191 - val_loss: 0.3673 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9173 - fn: 16.0000 - fp: 18.0000 - tn: 220.0000 - tp: 157.0000 - precision: 0.8971 - recall: 0.9075 - val_loss: 0.1710 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9416 - fn: 13.0000 - fp: 11.0000 - tn: 227.0000 - tp: 160.0000 - precision: 0.9357 - recall: 0.9249 - val_loss: 0.2965 - val_accuracy: 0.9416 - val_fn: 5.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 53.0000 - val_precision: 0.9464 - val_recall: 0.9138\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9270 - fn: 16.0000 - fp: 14.0000 - tn: 224.0000 - tp: 157.0000 - precision: 0.9181 - recall: 0.9075 - val_loss: 0.2670 - val_accuracy: 0.9197 - val_fn: 3.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 55.0000 - val_precision: 0.8730 - val_recall: 0.9483\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1790 - accuracy: 0.9221 - fn: 15.0000 - fp: 17.0000 - tn: 221.0000 - tp: 158.0000 - precision: 0.9029 - recall: 0.9133 - val_loss: 0.2086 - val_accuracy: 0.9270 - val_fn: 3.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 55.0000 - val_precision: 0.8871 - val_recall: 0.9483\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2008 - accuracy: 0.9148 - fn: 21.0000 - fp: 14.0000 - tn: 224.0000 - tp: 152.0000 - precision: 0.9157 - recall: 0.8786 - val_loss: 0.2713 - val_accuracy: 0.9197 - val_fn: 2.0000 - val_fp: 9.0000 - val_tn: 70.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.9655\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 6ms/step - loss: 0.1500 - accuracy: 0.9367 - fn: 16.0000 - fp: 10.0000 - tn: 228.0000 - tp: 157.0000 - precision: 0.9401 - recall: 0.9075 - val_loss: 0.3660 - val_accuracy: 0.9270 - val_fn: 2.0000 - val_fp: 8.0000 - val_tn: 71.0000 - val_tp: 56.0000 - val_precision: 0.8750 - val_recall: 0.9655\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1879 - accuracy: 0.9246 - fn: 13.0000 - fp: 18.0000 - tn: 220.0000 - tp: 160.0000 - precision: 0.8989 - recall: 0.9249 - val_loss: 0.2137 - val_accuracy: 0.9489 - val_fn: 4.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 54.0000 - val_precision: 0.9474 - val_recall: 0.9310\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1834 - accuracy: 0.9221 - fn: 18.0000 - fp: 14.0000 - tn: 224.0000 - tp: 155.0000 - precision: 0.9172 - recall: 0.8960 - val_loss: 0.2171 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9416 - fn: 13.0000 - fp: 11.0000 - tn: 227.0000 - tp: 160.0000 - precision: 0.9357 - recall: 0.9249 - val_loss: 0.2103 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9513 - fn: 13.0000 - fp: 7.0000 - tn: 231.0000 - tp: 160.0000 - precision: 0.9581 - recall: 0.9249 - val_loss: 0.2715 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9513 - fn: 9.0000 - fp: 11.0000 - tn: 227.0000 - tp: 164.0000 - precision: 0.9371 - recall: 0.9480 - val_loss: 0.2955 - val_accuracy: 0.9416 - val_fn: 4.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 54.0000 - val_precision: 0.9310 - val_recall: 0.9310\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9538 - fn: 11.0000 - fp: 8.0000 - tn: 230.0000 - tp: 162.0000 - precision: 0.9529 - recall: 0.9364 - val_loss: 0.3123 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1519 - accuracy: 0.9367 - fn: 21.0000 - fp: 5.0000 - tn: 233.0000 - tp: 152.0000 - precision: 0.9682 - recall: 0.8786 - val_loss: 0.3055 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9538 - fn: 10.0000 - fp: 9.0000 - tn: 229.0000 - tp: 163.0000 - precision: 0.9477 - recall: 0.9422 - val_loss: 0.3142 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9513 - fn: 10.0000 - fp: 10.0000 - tn: 228.0000 - tp: 163.0000 - precision: 0.9422 - recall: 0.9422 - val_loss: 0.3882 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9586 - fn: 9.0000 - fp: 8.0000 - tn: 230.0000 - tp: 164.0000 - precision: 0.9535 - recall: 0.9480 - val_loss: 0.2739 - val_accuracy: 0.9343 - val_fn: 6.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 52.0000 - val_precision: 0.9455 - val_recall: 0.8966\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1594 - accuracy: 0.9440 - fn: 15.0000 - fp: 8.0000 - tn: 230.0000 - tp: 158.0000 - precision: 0.9518 - recall: 0.9133 - val_loss: 0.2336 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9659 - fn: 8.0000 - fp: 6.0000 - tn: 232.0000 - tp: 165.0000 - precision: 0.9649 - recall: 0.9538 - val_loss: 0.4341 - val_accuracy: 0.9635 - val_fn: 1.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 57.0000 - val_precision: 0.9344 - val_recall: 0.9828\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9416 - fn: 10.0000 - fp: 14.0000 - tn: 224.0000 - tp: 163.0000 - precision: 0.9209 - recall: 0.9422 - val_loss: 0.3634 - val_accuracy: 0.9562 - val_fn: 1.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 57.0000 - val_precision: 0.9194 - val_recall: 0.9828\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9586 - fn: 10.0000 - fp: 7.0000 - tn: 231.0000 - tp: 163.0000 - precision: 0.9588 - recall: 0.9422 - val_loss: 0.2894 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9270 - fn: 16.0000 - fp: 14.0000 - tn: 224.0000 - tp: 157.0000 - precision: 0.9181 - recall: 0.9075 - val_loss: 0.3579 - val_accuracy: 0.9197 - val_fn: 8.0000 - val_fp: 3.0000 - val_tn: 76.0000 - val_tp: 50.0000 - val_precision: 0.9434 - val_recall: 0.8621\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2308 - accuracy: 0.9124 - fn: 12.0000 - fp: 24.0000 - tn: 214.0000 - tp: 161.0000 - precision: 0.8703 - recall: 0.9306 - val_loss: 0.2629 - val_accuracy: 0.9124 - val_fn: 5.0000 - val_fp: 7.0000 - val_tn: 72.0000 - val_tp: 53.0000 - val_precision: 0.8833 - val_recall: 0.9138\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9173 - fn: 20.0000 - fp: 14.0000 - tn: 224.0000 - tp: 153.0000 - precision: 0.9162 - recall: 0.8844 - val_loss: 0.3250 - val_accuracy: 0.9197 - val_fn: 2.0000 - val_fp: 9.0000 - val_tn: 70.0000 - val_tp: 56.0000 - val_precision: 0.8615 - val_recall: 0.9655\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9294 - fn: 20.0000 - fp: 9.0000 - tn: 229.0000 - tp: 153.0000 - precision: 0.9444 - recall: 0.8844 - val_loss: 0.3417 - val_accuracy: 0.9489 - val_fn: 3.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 55.0000 - val_precision: 0.9322 - val_recall: 0.9483\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9440 - fn: 9.0000 - fp: 14.0000 - tn: 224.0000 - tp: 164.0000 - precision: 0.9213 - recall: 0.9480 - val_loss: 0.3392 - val_accuracy: 0.9562 - val_fn: 2.0000 - val_fp: 4.0000 - val_tn: 75.0000 - val_tp: 56.0000 - val_precision: 0.9333 - val_recall: 0.9655\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 5ms/step - loss: 0.1656 - accuracy: 0.9173 - fn: 19.0000 - fp: 15.0000 - tn: 223.0000 - tp: 154.0000 - precision: 0.9112 - recall: 0.8902 - val_loss: 0.3414 - val_accuracy: 0.9416 - val_fn: 3.0000 - val_fp: 5.0000 - val_tn: 74.0000 - val_tp: 55.0000 - val_precision: 0.9167 - val_recall: 0.9483\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    'accuracy',\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\n",
    "\n",
    "\n",
    "history=model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=10,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'fn', 'fp', 'tn', 'tp', 'precision', 'recall', 'val_loss', 'val_accuracy', 'val_fn', 'val_fp', 'val_tn', 'val_tp', 'val_precision', 'val_recall'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.942\n",
      "Recall:  0.948\n",
      "Precision:  0.917\n",
      "ROC AUC:  0.942\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test, model.predict_classes(X_test)\n",
    "\n",
    "accuracy = round(accuracy_score(y_true, y_pred),3)\n",
    "recall = round(recall_score(y_true, y_pred),3)\n",
    "precision = round(precision_score(y_true, y_pred),3)\n",
    "roc_auc = round(roc_auc_score(y_true, y_pred),3)\n",
    "\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Recall: ',recall)\n",
    "print('Precision: ',precision)\n",
    "print('ROC AUC: ',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
